<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="CSCI">
  <meta property="og:title" content="ICCV"/>
  <meta property="og:description" content="Person ReID biometrics survillence ICCV"/>
  <meta property="og:url" content="https://github.com/ppriyank/ICCV-CSCI-Person-ReID"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/banner.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement">
  <meta name="twitter:description" content="Person ReID biometrics survillence ICCV">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner.jpeg">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Person ReID biometrics survillence ICCV">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement</title>
  <link rel="icon" type="image/x-icon" href="static/images/banner.jpeg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=wnVyeJ0AAAAJ&hl=en" target="_blank">Priyank Pathak</a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=D_JvEcwAAAAJ&hl=en" target="_blank">Yogesh Singh Rawat</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Central Florida<br><b style="color:red;">ICCV 2025 ü•≥</b></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ppriyank/ICCV-CSCI-Person-ReID" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">


      <div style="display: flex; flex-wrap: wrap; justify-content: center; align-items: center; gap: 10px;">
        <img src="static/images/colors_Gif/vid 2.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 3.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 4.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 5.gif" width="11%" >
        <img src="static/images/colors_Gif/vid.gif" width="11%"  >
        <img src="static/images/colors_Gif/vid 6.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 7.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 8.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 9.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 10.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 11.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 12.gif" width="11%">
        <img src="static/images/colors_Gif/vid 13.gif" width="11%">
        <img src="static/images/colors_Gif/vid 14.gif" width="11%">
        <img src="static/images/colors_Gif/vid 15.gif" width="11%">
        <img src="static/images/colors_Gif/vid 16.gif" width="11%">
        </div>
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        Clustering using color histograms groups images/video frames based on people wearing the same clothes. Video frames taken from the CCVID dataset. This suggests that colors can serve as a proxy for clothing labels.
      </h2>

    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract üòè</h2>
        <div class="content has-text-justified">
          <p>
            Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals across different locations and times, irrespective of clothing. 
            Existing methods often rely on additional models or annotations to learn robust, clothing-invariant features, making them resource-intensive. 
            In contrast, we explore the use of color‚Äîspecifically foreground and background colors‚Äîas a lightweight, annotation-free proxy for mitigating appearance bias in ReID models.
            We propose <b>Colors See, Colors Ignore (CSCI)</b>, a RGB-only method that leverages color information directly from raw images or video frames. CSCI efficiently captures color-related appearance bias (<b>'Color See'</b>) while disentangling it from identity-relevant ReID features (<b>'Color Ignore'</b>). To achieve this, we introduce <b>S2A self-attention</b>, a novel self-attention to prevent information leak between color and identity cues within the feature space. Our analysis shows a strong correspondence between learned color embeddings and clothing attributes, validating color as an effective proxy when explicit clothing labels are unavailable.
            We demonstrate the effectiveness of CSCI on both image and video ReID with extensive experiments on four CC-ReID datasets. 
            We improve baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for image-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID without relying on additional supervision.  Our results highlight the potential of color as a cost-effective solution for addressing appearance bias in CC-ReID..
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- METHOD -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method üòé</h2>
        <img src="static/images/csci.gif" alt="MY ALT TEXT" width="80%"/>
        <div class="content has-text-justified">
          <p>
            Traditional transformer-based ReID models use RGB spatial pages of input and pass them through layers of transformers. The class token is used as <b style="color:rgb(13, 141, 163);">ReID Token</b> for inference and is trained using triplet loss, an identity-based classifier. 
            We introduce one additional class token, called <b style="color:rgb(255, 0, 0);">Color Token</b>, for which learns color embedding via MSE (regression) on color histograms. 
            We then disentangle Color Token from ReID token using cosine loss.
          </p>
        </div>
        <!-- <br><br><br><br> -->
        <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <div class="image-container">
                  <img src="static/images/SA1.jpg" class="equal-image" />
                </div>
                <p class="carousel-caption">
                  <b>Traditional Self-Attention</b> shares information across all tokens, leaking information across ReID (biometrics) and Color tokens (appearance bias). This is an example of <b>100% overlap</b> between biometrics and appearance bias.
              </div>
            </div>

            <div class="column">
              <div class="content">
                <div class="image-container">
                  <img src="static/images/SA2.jpg" class="equal-image" />
                </div>
                <p class="carousel-caption">
                  <b>Masked Self-Attention</b> doesn't allow information sharing across ReID (biometrics) and Color tokens (appearance bias); however, ReID and Color tokens influence the weight of each other on spatial tokens. This is an example of <b>0% overlap</b> between biometrics and appearance bias, but they influence each other's weight.
              </div>
            </div>

            <div class="column">
              <div class="content">
                <div class="image-container">
                  <img src="static/images/SA3.jpg" class="equal-image" />
                </div>
                <p class="carousel-caption">
                  <b style="color:rgb(255, 0, 0);" >S2A Self-Attention (Ours ‚ò∫Ô∏è)  </b> By doing two-step self-attention, Color tokens (appearance bias) no longer influence the weight of the ReID tokens (biometrics) and vice-versa. This is an example of <b>0% overlap</b> between biometrics and appearance bias. By adjusting the weights of the averaging of the spatial tokens (one aware of biometrics), and the other aware of appearance bias, we can influence which signal gets more weightage (biometrics or appearance bias). Current hyperpater is set to 0.5 equal weight for both.
              </div>
            </div>
        </div>
        <div class="carousel-caption">
          <h3 class="title is-3"> <b>Alternative üò±</b></h3>
            Alternative to our S2A self-attention would be to use two transformers, one for ReID and the other for appearance bias, which is computationally impractical for deployment. An alternative to using color would be to use clothing integer annotations instead of colors. However, colors are more expressive than integer clothing labels. Another alternative would be that LLM-based fine-grained clothing description would be computationally infeasible, and needs to be generated per frame on video, as clothing may change across video.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- -->


<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Results üßê</h2>
      Numbers reported are an average of two runs, however pretrained weights are provided for the best performing models which has much higher accuracy than the reported values.
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
         <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/generalization.png" width="50%" />
        </div>
        <h2 class="subtitle has-text-centered">
          <br><b>Generalization</b> <br>CSCI apporach generalizes across various previous works involving transformers and consistently outperforms traditional integer clothing labels.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/img_result.png" width="80%" />
        </div>
        <h2 class="subtitle has-text-centered">
          <br><b>Image ReID Results</b> <br>CSCI doesn't require any annotation or supervision, unlike previous works. 
        </h2>
      </div>
      
      <div class="item">
        <!-- Your image here -->
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/video_result.png" width="80%" />
        </div>
        <h2 class="subtitle has-text-centered">
         <br><b>Video ReID Results</b> <br>CSCI doesn't require any annotation or supervision, unlike previous works. EZ-CLIP singnificantly improves Video ReID performance.
       </h2>
      </div>

      <div class="item">
        <!-- Your image here -->
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/self-attention.png" width="40%" />
        </div>
        <h2 class="subtitle has-text-centered">
         <br><b>Self-Attention alternatives</b> <br>S2A self-attention outperforms Masked and traditional self-attention, indicating the strong need for preventing 1) information leak between biometrics signals and appearance bias. 2) Preventing appearance bias influence on the weight of biometrics signals and vice versa. 
       </h2>
      </div>

      <div class="item">
        <!-- Your image here -->
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/color_variation.png" width="20%" />
        </div>
        <h2 class="subtitle has-text-centered">
         <br><b>Alternatives to clothes</b><br> Colors outperform tradtional integer clothing annotations and grey colored inputs.
       </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Ablation -->
<section class="section hero">
  <div class="container">
    <h2 class="title is-3">Ablation ü•∏</h2>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <div class="image-container2">
                  <img src="static/images/SCDK.png" class="equal-image2" />
                </div>
                <p class="carousel-caption">
                  Same clothes labels but different K means cluster of color embeddings indicates models take into account illumination, and other environmental factors. Depending on how you see it, it can be a <b>limitation</b> as "noisy clothing labels" or color embeddings indicate what's happening in the exact moment, aka <b>"Adaptive"</b>. 
                </p>
              </div>
            </div>

            <div class="column">
              <div class="content">
                <div class="image-container2">
                  <!--   -->
                  <img src="static/images/DCSK.png" class="equal-image2" />
                </div>
                <p class="carousel-caption">
                  Different clothes but similar K-means cluster of color embeddings indicate the true  <b>limitation</b> of colors. Colors take into account overall frames, which may make models relate different images based on similar clothing foreground and background colors. Solution : horizontal / vertical splits of colors / fine-grained colors?
              </div>
            </div>

            <!-- <div class="column">
              <div class="content">
                <div class="image-container">
                  <img src="static/images/KMeans.png" class="equal-image" />
                </div>
                <p class="carousel-caption">
                  <b style="color:rgb(255, 0, 0);" >S2A Self-Attention (Oursü•Ç)  </b> By doing two-step self-attention, Color tokens (appearance bias) no longer influence the weight of the ReID tokens (biometrics) and vice-versa. This is an example of <b>0% overlap</b> between biometrics and appearance bias. By adjusting the weights of the averaging of the spatial tokens (one aware of biometrics), and the other aware of appearance bias, we can influence which signal gets more weightage (biometrics or appearance bias). Current hyperpater is set to 0.5 equal weight for both.
               </div> -->
            </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- -->


<hr class="divider" />
<hr class="divider" />
<!--BibTex citation -->
  <section class="section is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX ü•π</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the<a href="https://nerfies.github.io" target="_blank">Nerfies</a>project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
